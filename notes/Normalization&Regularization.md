# Lecture 9

*DATE: 2022/11/ 10*
---

**Batch normalization**

- *Internal covariate shift*: distribution of each layerâ€™s inputs changes during
  training, which slows down the training. 
- allows much higher **learning rates** and be less careful about **initialization**. Also acts as a regularizer, eliminating the need for dropout.
- 

